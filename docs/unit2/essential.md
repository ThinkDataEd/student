##**IDS Unit 2: Essential Concepts**

###**<u>[Lesson 1: What Is Your True Color?](lesson1.md)</u>**
Students will understand that the 'typical' value is a value that can represent the entire group, even
though we know that not all members of the group share the same value.

###**<u>[Lesson 2: What Does Mean Mean?](lesson2.md)</u>**
The center of a distribution is the 'typical' value. One way of measuring the center is with the mean,
which finds the balancing point of the distribution. The mean gives us the typical value, but does not tell
the whole story. We need a way to measure the variability to understand how observations might differ
from the typical value.

###**<u>[Lesson 3: Median In the Middle](lesson3.md)</u>**
Another measure of center is the median, which can also be used to represent the typical value of a
distribution. The median is preferred for skewed distributions or when there are outliers, because it better
matches what we think of as 'typical.'

###**<u>[Lesson 4: How Far Is It from Typical?](lesson4.md)</u>**
MAD measures the variability in a sample of data - the larger the value, the greater the variability. More
precisely, the MAD is the typical distance of observations from the mean. There are other measures of
spread as well, notably the standard deviation and the interquartile range (IQR).

###**<u>[Lesson 5: Human Boxplots](lesson5.md)</u>**
A common statistical question is “How does this group compare to that group?” This is a hard question to
answer when the groups have lots of variability. One approach is to compare the centers, spreads, and
shapes of the distributions. Boxplots are a useful way of comparing distributions from different groups
when all of the distributions are unimodal (one hump).

###**<u>[Lesson 6: Face Off](lesson6.md)</u>**
Writing (and saying) precise comparisons between groups in which variability is present based on the (a)
center, (b) spread, (c) shape, and (d) unusual outcomes help to make statements in context of the data.
Actual comparison statements should use terms such as "less than," "about the same as," etc.

###**<u>[Lesson 7: Plot Match](lesson7.md)</u>**
Boxplots are an alternative visualization of histograms or dot plots. They capture most, but not all, of the
features we can see in a dot plot or histogram.

###**<u>[Lesson 8: How Likely Is It?](lesson8.md)</u>**
Probability is an area about which we humans have poor intuition. Probability measures a long-run
proportion: 50% chance means the event happens 50% of the time if you repeated it forever. When we
don't repeat forever, we see variability.

###**<u>[Lesson 9: Bias Detective](lesson9.md)</u>**
In the short-term, actual outcomes of chance experiments vary from what is 'ideal.' An ideal die has
equally likely outcomes. But that does not mean we will see exactly the same number of one dots, two
dots, etc.

###**<u>[Lesson 10: Marbles, Marbles…](lesson10.md)</u>**
There are two ways of sampling data that model real-life sampling situations: with and without
replacement. Larger samples tend to be closer to the "true" probability.

###**<u>[Lesson 11: This AND/OR That](lesson11.md)</u>**
What does "A or B" mean versus "A and B" mean? These are compound events and two-way tables can
be used to calculate probabilities for them.

###**<u>[Lesson 12: Don’t Take My Stress Away!](lesson12.md)</u>**
Generating statistical questions is the first step in a Participatory Sensing campaign. Research and
observations help create applicable campaign questions.

###**<u>[Lesson 13: The Horror Movie Shuffle](lesson13.md)</u>**
We can "shuffle" data based on categorical variables. The statistic we use is the difference in proportions.
The distribution we form by shuffling represents what happens if chance were the only factor at play. If
the actual observed difference in proportions is near the center of this shuffling distribution, then we would
conclude that chance is a good explanation for the difference. But if it is extreme (in the tails or off the
charts), then we should conclude that chance is NOT to blame. Sometimes, the apparent difference
between groups is caused by chance.

###**<u>[Lesson 14: The Titanic Shuffle](lesson14.md)</u>**
We can also "shuffle" data based on numerical variables. The statistic we use is the difference in means.
The distribution we form by this form of shuffling still represents what happens if chance were the only
factor at play. When differences are small, we suspect that they might be due to chance. When
differences are big, we suspect they might be 'real.'

###**<u>[Lesson 15: Tangible Data Merging](lesson15.md)</u>**
We can enhance the context of a statistical problem by merging related data sets together. To merge
data, each data set must have a "unique identifier" that tells us how to match up the lines of the data.

###**<u>[Lesson 16: What Is Normal?](lesson16.md)</u>**
The Normal curve, also called the Gaussian distribution and the "bell curve," is a model that describes
many real-life distributions and is usually called the Normal Model.

###**<u>[Lesson 17: A Normal Measure of Spread](lesson17.md)</u>**
The standard deviation is another measure of spread. This is commonly used by statisticians because of
its role in common models and distributions, such as the Normal Model.

###**<u>[Lesson 18: Shuffling with Normal](lesson18.md)</u>**
Z-scores allow us a way to measure how extreme a value is, regardless of the units of measurement.
Usually, z-scores will range between -3 and +3, and so values that are at or more extreme than -3 or +3
standard deviations are considered large.